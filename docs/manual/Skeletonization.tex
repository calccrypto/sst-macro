% !TEX root = manual.tex

\chapter{Applications and Skeletonization}
\label{chap:appsAndSkeletonization}

\section{Basic Application porting}
\label{sec:skel:basic}
There are three parts to successfully taking a C++ code and turning it into a running application.
\begin{itemize}
\item Redirected linkage: Rather than linking to MPI, pThreads, or other parallel libraries (or even calling \inlinecode{hostname}), these functions must be redirected to \sstmacro rather than actually executing.
You get all redirected linkage for free by using
the SST compiler wrappers \inlineshell{sst++} and \inlineshell{sstcc} installed in the \inlineshell{bin} folder.
\item Skeletonization: While \sstmacro can run in emulation mode, executing your entire application exactly, this is not scalable.  To simulate at scale (i.e. 100K or more MPI ranks) you must strip down or ``skeletonize'' the application to the minimal amount of computation.  The energy and time cost of expensive compute kernels are then simulated via models rather than explicitly executed. 

\item Process encapsulation: Each virtual process being simulated is not an actual physical process. It is instead modeled as a lightweight user-space thread.  This means each virtual process has its own stack and register variables, but that is it.
Virtual processes share the same address space and the same global variables.  Some refactoring may be necessary if you have global variables.

\end{itemize}

\section{Redirected linkage}
\label{sec:skel:linkage}
Because of the way libraries are import in the SST core, using external skeleton apps is more complicated than previously.  Steps are being taken to re-automate this process, which will hopefully be available in the next subversion. For running the standalone version, this is essentially automatic  if using the SST compiler wrappers.  The compiler wrappers produce a new executable incorporating your new skeleton.

The wrinkle is external skeletons apps is changing the application's entry point, i.e. \inlinecode{main}.
The SST/macro framework has already taken the \texttt{main} function, and consequently the user application becomes a sub-routine within the simulation environment. As introduced in Section~\ref{sec:tutorial:basicmpi}, one needs redirect \texttt{main} to a different symbol by defining the macro \inlinecode{sstmac_app_name} at the top of the file containing \inlinecode{main}. This refactoring happens automatically in the SST compiler wrappers. 


\subsection{Loading external skeletons with the integrated core}\label{subsec:linkageCore}
While the main \inlineshell{libmacro.so} provides the bulk of SST/macro functionality, 
users may wish to compile and run external skeletons.  This gets a bit confusing with SST core since you have an external skeleton for an external element.  One can do this manually following the instructions on \url{http://sst-simulator.org}. You must create an element info struct name \inlinecode{X_eli} for X your library name.  You can still use the \inlineshell{sst++} compiler wrappers for building, but you must now manually create a \inlineshell{libX.so} from the compiled object files.  The \inlineshell{default.py} script used by \inlineshell{pysstmac} must also be edited.  The top lines was previously

\begin{ViFile}
import sst.macro
\end{ViFile}
This only loads the main components, not the external skeleton. You must add

\begin{ViFile}
import sst.X
\end{ViFile}
where X is the name of your skeleton. This causes the core to also load the shared library corresponding to your external skeleton app.
If using the SST compiler wrappers, the ELI block and .so file will actually be generated automatically.  As shown in Section \ref{sec:building:running},
the generate shared library files can be added as the first parameters to the \inlineshell{pysstmac} script.

\begin{ShellCmd}
>sst-macro/skeletons/sendrecv> pysstmac librunsstmac.so -f parameters.ini
\end{ShellCmd} 


\section{Skeletonization}
\label{sec:skeletonization}

A program skeleton is a simplified program derived from a parent application. The purpose of a skeleton application is to retain the performance characteristics of interest. At the same time, program logic that is orthogonal to performance properties is removed.  The rest of this chapter will talk about skeletonizing an MPI program, but the concepts mostly apply regardless of what programming/communication model you're using. 

The default method for skeletonizing an application is \textit{manually}. In other words, going through your application and removing all the computation that is not necessary to produce the same communication/parallel characteristics.   Essentially, what you're doing is visually backtracing variables in MPI calls to where they are created, and removing everything else.  

Skeletonization falls into three main categories:

\begin{itemize}
\item \textit{Data structures} - Memory is a precious commodity when running large simulations, so get rid of every memory allocation you can.
\item \textit{Loops} - Usually the main brunt of CPU time, so get rid of any loops that don't contain MPI calls or calculate variables needed in MPI calls.
\item \textit{Communication buffers} - While you can pass in real buffers with data to \sstmacro MPI calls and they will work like normal, it is relatively expensive. If they're not needed, get rid of them.
\end{itemize}

A good example of skeletonization can be found in the lulesh2.0.3 example in the skeletons folder.

\subsection{Basic compute modeling}
\label{subsec:basicCompute}

By default, even if you don't remove any computation, \textit{simulation time doesn't pass between MPI or other calls implemented by \sstmacro} unless you set

\begin{ViFile}
host_compute_modeling = true
\end{ViFile}

in your parameter file.  In this case, \sstmacro will use the wall time that the host takes to run code between MPI calls and use that as simulated time.  This only makes sense, of course, if you didn't do any skeletonization and the original code is all there. 

If you do skeletonize your application and remove computation, you need to replace it with a model of the time or resources necessary to perform that computation so that \sstmacro can advance simulation time properly. 
These functions are all accessible by using the SST compiler wrappers or by adding \inlinefile{#include <sstmac/compute.h>} to your file.

You can describe the time it takes to do computation by inserting calls to 

\begin{ViFile}
void sstmac_compute(double seconds)
\end{ViFile}
Usually, this would be parameterized by some value coming from the application, like loop size.   You can also describe memory movement with 

\begin{ViFile}
void sstmac_memread(long bytes);
void sstmac_memwrite(long bytes);
void sstmac_memcpy(long bytes);
\end{ViFile}
again usually parameterized by something like vector size.  
Using these two functions is the simplest and least flexible way of compute modeling.

\subsection{Detailed compute modeling}
\label{subsec:detailedCompute}

The basic compute modeling is not very flexible.  
In particular, simply computing based on time does not account for congestion delays introduced by things like memory contention.
The highly recommended route is a more detailed compute model (but still very simple) that uses the operational intensity (essentially bytes/flops ratio) for a given compute kernel.
This informs \sstmacro how much stress a given code region puts on either the processor or the memory system.
If a kernel has a very high operational intensity, then the kernel is not memory-bound.
The means multiple threads can be running the kernel with essentially no memory contention.
If a kernel has a very low operational intensity, the kernel is memory bound.
A single thread will have good performance, but multiple threads will compete heavily for memory bandwidth.
If a kernel has a medium operational intensity, a few concurrent threads may be possible without heavy contention, 
but as more threads are added the contention will quickly increase.

The function prototype is

\begin{CppCode}
void
sstmac_compute_detailed(long nflops, long nintops, long bytes);
\end{CppCode}
Here \inlinecode{flops} is the number of floating point operations and \inlinecode{bytes} is the number of bytes that hit the memory controller.
\inlinecode{bytes} is \emph{not} simply the number of writes/reads that a kernel performs.
This is the number of writes/reads that \emph{miss the cache} and hit the memory system.
For now, \sstmacro assumes a single-level cache and does not distinguish between L1, L2, or L3 cache misses.
Future versions may incorporate some estimates of cache hierarchies.
However, given the coarse-grained nature of the simulation, explicit simulation of cache hierarchies is not likely to provide enough improved accuracy or physical insight to justify the increased computational cost. 
Additional improvements are likely to involve adding parameters for pipelining and prefetching.
This is currently the most active area of \sstmacro development.

The characterization of a compute kernel must occur outside \sstmacro using performance analysis tools like Vtune or PAPI.
For the number of flops, it can be quite easy to just count the number of flops by hand.
The number of bytes is much harder.
For simple kernels like a dot product or certain types of stencil computation, 
it may be possible to pen-and-paper derive estimates of the number of bytes read/written from memory since every read is essentially a cache miss.
In the same way, certain kernels that use small blocks (dense linear algebra), it may be possible to reason \textit{a priori} about the cache behavior.
For more complicated kernels, performance metrics might be the only way.
Further discussion and analysis of operational intensity and roofline models can be found in ``Roofline Model Toolkit: A Practical Tool for Architectural and Program Analysis'' by Yung Ju Lo et al.  The PDF is available at \url{http://www.dcs.warwick.ac.uk/~sdh/pmbs14/PMBS14/Workshop_Schedule.html}.

\subsection{Skeletonization Issues}
\label{subsec:skeletonIssues}

The main issue that arises during skeletonization is data-dependent communication.  In many cases, it will seem like you can't remove computation or memory allocation because MPI calls depend somehow on that data.  The following are some examples of how we deal with those:

\begin{itemize}
\item \textit{Loop convergence} - In some algorithms, the number of times you iterate through the main loop depends on an error converging to near zero, or some other converging mechanism.  This basically means you can't take out anything at all, because the final result of the computation dictates the number of loops.  In this case, we usually set the number of main loop iterations to a fixed (parameterized) number.  Do we really care exactly how many loops we went through?  Most of the time, no, it's enough just to produce the behavior of the application.  
\item \textit{Particle migration} - Some codes have a particle-in-cell structure, where the spatial domain is decomposed among processes, and particles or elements are distributed among them, and forces between particles are calculated.  When a particle moves to another domain/process (because it's moving through space), this usually requires communication that is different from the force calculation, and thus depends entirely on the data in the application.  We can handle this in two ways: 1) \textit{Ignore it} - If it doesn't happen that often, maybe it's not significant anyway.  So just remove the communication, recognizing that the behavior of the skeleton will not be fully reproduced or 2) \textit{Approximate it} - If all we need to know is that this migration/communication happens sometimes, then we can just make it happen every so many iterations, or even sample from a probability distribution.  
\item \textit{AMR} - Some applications, like adaptive mesh refinement (AMR), exhibit communication that is entirely dependent on the computation.  In this case, skeletonization is basically impossible, so you're left with a few options
\end{itemize}

For applications with heavy dynamic data dependence, we have the following strategies:
\begin{itemize}
\item \textit{Traces}  - revert to DUMPI traces, where you will be limited by existing machine size.  Trace extrapolation is also an option here.
\item \textit{Run it} - get yourself a few servers with a lot of memory, and run the whole code in \sstmacro.
\item \textit{Synthetic} - It may be possible to replace communication with randomly-generated data and decisions, which emulate how the original application worked. This hasn't been tried yet.
\item \textit{Hybrid} - It is possible to construct meta-traces that describe the problem from a real run, and read them into \sstmacro to reconstruct the communication that happens.  Future versions of this manual will have more detailed descriptions as we formalize this process.
\end{itemize}

\section{Process Encapsulation}
\label{sec:processEncapsulation}

As mentioned above, virtual processes are not real, physical processes inside the OS.
They are explicitly managed user-space threads with a private stack, but without a private set of global variables.
When porting an application to SST/macro, global variables used in C programs will not be mapped to separate memory addresses causing incorrect execution or even segmentation faults.
If you have avoided global variables, there is no major issue.  
If you have read-only global variables with the same value on each machine, there is still no issue.
If you have mutable global variables or read-only variables such as \inlinecode{mpi_rank} that differ across processes,
there is so minor refactoring that needs to be done.

\subsection{Manually refactoring global variables}
\label{sec:skel:globals}

\sstmacro provides a complete set of global variable replacements from 
\inlinecode{\#include <sstmac/sstmac_global.h>}, which is automatically included the SST compiler wrappers.
Then replace the variable type declaration with the ones that have a \inlinecode{global_} prefix in the header file.
To use this file, you must compile your application with a C++ compiler as a C++ program.  While most of C++ is backwards-compatible, there are some things that are not, and will require either a compiler flag to relax strictness or quick refactor of some of your syntax.

When printing a global variable with \inlinecode{printf}, the user should explicitly invoke a cast to the primitive type in the function call:

\begin{CppCode}
print("Hello world on rank %d", int(rank));
\end{CppCode}
If not explicitly cast, the \inlinecode{va_args} function will be misinterpreted and produce an ``Illegal instruction'' error.  
This still follows the ``single-source'' principle since whether compiling for \sstmacro or a real machine, the code is still valid.

\subsection{Automatically refactoring global variables}
\label{subsec:autoRefactorGlobals}

Tools are currently in use by developers to automatically refactor codes to use no global variables.
This involves running the source code through a compiler tool chain that then creates a \inlinecode{struct}
encapsulating all global variables into thread-specific classes.
This process is only for advanced users and requires developer help.


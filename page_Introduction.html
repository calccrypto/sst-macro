<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>SST/macro: Introduction</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customthing.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">SST/macro
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Introduction </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="sec_intro_overview"></a>
Overview</h1>
<p>The SST/macro software package provides a simulator for large-scale parallel computer architectures. It permits the coarse-grained study of distributed-memory applications. The simulator is driven from either a trace file or skeleton application. The simulator architecture is modular, allowing it to easily be extended with additional network models, trace file formats, software services, and processor models.</p>
<p>Simulation can be broadly categorized as either off-line or on-line. Off-line simulators typically first run a full parallel application on a real machine, recording certain communication and computation events to a simulation trace. This event trace can then be replayed post-mortem in the simulator. Most common are MPI traces which record all MPI events, and SST/macro provides the DUMPI utility (<a class="el" href="page_DumpiTutorial.html#sec_tutorial_dumpi">Using DUMPI</a>) for collecting and replaying MPI traces. Trace extrapolation can extend the usefulness of off-line simulation by estimating large or untraceable system scales without having to collect a trace, it is typically only limited to strictly weak scaling.</p>
<p>We turn to on-line simulation when the hardware or applications parameters need to change. On-line simulators instead run real application code, allowing native C/C++/Fortran to be compiled directly into the simulator. SST/macro intercepts certain function calls, estimating how much time passes rather than actually executing the function. In MPI programs, for example, calls to MPI_Send are linked to the simulator instead of passing to the real MPI library. If desired, SST/macro can actually be a full MPI <em>emulator</em>, delivering messages between ranks and replicating the behavior of a full MPI implementation.</p>
<p>Although SST/macro supports both on-line and off-line modes, on-line simulation is encouraged because event traces are much less flexible, containing a fixed sequence of events. Application inputs and number of nodes cannot be changed. Without a flexible control flow, it also cannot simulate dynamic behavior like load balancing or faults. On-line simulation can explore a much broader problem space since they evolve directly in the simulator.</p>
<p>For large, system-level experiments with thousands of network endpoints, high-accuracy cycle-accurate simulation is not possible, or at least not convenient. Simulation requires coarse-grained approximations to be practical. SST/macro is therefore designed for specific cost/accuracy tradeoffs. It should still capture complex cause/effect behavior in applications and hardware, but be efficient enough to simulate at the system-level. For speeding up simulator execution, we encourage <em>skeletonization</em>, discussed further in in the PDF manual. A high-quality skeleton is an application model that reproduces certain characteristics with only limited computation. We also encourage uncertainty quantification (UQ) for validating simulator results, discussed further in in the PDF manual. Skeletonization and UQ are the two main elements in the "canonical'' SST/macro workflow (Figure 1).</p>
<p><br />
</p>
<pre class="fragment">\image html figures/workflow.png
  &lt;b&gt;Figure 1:&lt;/b&gt; SST/macro workflow. 
</pre><p><br />
<br />
</p>
<h1><a class="anchor" id="sec_intro_supported"></a>
Currently Supported</h1>
<h2><a class="anchor" id="subsec_intro_apis"></a>
Programming APIs</h2>
<p>The following sections describe the state of the software API's (found in <em>sstmac</em>) that are available in SST/macro for use by applications, as of this release. The level of testing indicates the integration of compliance/functionality tests into our make check test suite.</p>
<h3><a class="anchor" id="subsubsec_intro_tested"></a>
Final and Tested</h3>
<ul>
<li>
{MPI:} Because of its popularity, MPI is one of our main priorities in providing programming model support. We currently test against the MPICH test suite. All tests compile, so you should never see compilation errors. However, since many of the functions are not typically used in the community, we only test commonly-used functions. See Section <a class="el" href="page_Introduction.html#subsec_issues_mpi">MPI</a> for functions that are not supported. Functions that are not implemented will throw a <em>sstmac::unimplemented_error</em>, reporting the function name. </li>
<li>
<p class="startli">{OpenSHMEM:} Most of the standard OpenSHMEM tests pass. The ones that don't are because they haven't been ported to C++, or test the single unsupported feature (collect).</p>
<p class="endli"></p>
</li>
</ul>
<h3><a class="anchor" id="subsubsec_intro_sometesting"></a>
Some testing complete</h3>
<ul>
<li>
{HPX:} HPX is an implementation of the Parallex execution model. Some applications have been ported to it, and it has a simple test in the make check suite. Further development and test integration of HPX is not likely. </li>
<li>
<p class="startli">{Sockets:} The Socket API is mostly implemented. Most basic client/server functionality is available. However, only the default socket options are allowed. In most cases, <code>setsockopt</code> is just a no-op.</p>
<p class="endli"></p>
</li>
</ul>
<h3><a class="anchor" id="subsubsec_intro_inutero"></a>
In development</h3>
<ul>
<li>
{Pthreads:} Only the <code><a class="el" href="sstmac__pthread__macro_8h.html#a3c7d37cba2d420a45309a93b799d9db9">pthread_create()</a></code>, <code><a class="el" href="sstmac__pthread__macro_8h.html#a0455e1829cb48db441a702e31229bc15">pthread_join()</a></code>, and <code><a class="el" href="sstmac__pthread__macro_8h.html#a5639a9f7ffbc15267fc5a03474a50bd6">pthread_self()</a></code> functions are implemented. A basic pthread test validates the core spawn/run/join behavior. </li>
<li>
{UPC:} We almost have the full UPC build and runtime implemented, but no tests are currently integrated and there are many bugs to work out before it can be used. </li>
<li>
<p class="startli">{GNI:} Cray's low-level messaging interface, GNI, is being implemented.</p>
<p class="endli"></p>
</li>
</ul>
<h2><a class="anchor" id="subsec_intro_toolsandstats"></a>
Analysis Tools and Statistics</h2>
<p>The following analysis tools are currently available in SST/macro. Some are thoroughly tested. Others have undergone some testing, but are still considered Beta. Others have been implemented, but are relatively untested. </p>
<h3><a class="anchor" id="subsubsec_intro_fulltestedtools"></a>
Fully tested</h3>
<ul>
<li>
Call graph: Generates callgrind.out file that can be visualized in either KCacheGrind or QCacheGrind. More details are given in <a class="el" href="page_CallGraphTutorial.html#sec_tutorials_callgraph">Call Graph Visualization</a>. </li>
<li>
Spyplot: Generates .csv data files tabulating the number of messages and number of bytes sent between MPI ranks. SST/macro can also directly generate a PNG file. Otherwise, the .csv files can be visualized in the plotting program Scilab. More details are given in <a class="el" href="page_SpyPlotTutorial.html#sec_tutorials_spyplot">Spyplot Diagrams</a>. </li>
<li>
<p class="startli">Fixed-time quanta (FTQ): Generates a .csv data tabulating the amount of time spent doing computation/communication as the application progresses along with a Gnuplot script for visualization as a histogram. More details are given in <a class="el" href="page_FTQTutorial.html#sec_tutorials_ftq">Fixed-Time Quanta Charts</a></p>
<p class="endli"></p>
</li>
</ul>
<h3><a class="anchor" id="subsubsec_intro_betatools"></a>
Beta</h3>
<ul>
<li>
<p class="startli">Trace analysis: With the traceanalyzer executable, fine-grained metrics for characterizing application execution can be output.</p>
<p class="endli"></p>
</li>
</ul>
<h3><a class="anchor" id="subsubsec_intro_untestedtools"></a>
Untested</h3>
<ul>
<li>
<p class="startli">Congestion: With the <code>-d "&lt;stats&gt; congestion"</code> command line option, SST/macro will dump statistics for network congestion on individual links (packet train model only).</p>
<p class="endli"></p>
</li>
</ul>
<h1><a class="anchor" id="sec_intro_issues"></a>
Known Issues and Limitations</h1>
<h2><a class="anchor" id="subsec_issues_globals"></a>
Global Variables</h2>
<p>The use of global variables in SST/macro inherently creates a false-sharing scenario because of the use of user-space threads to model parallel processes. While we do have a mechanism for supporting them (in the PDF manual for more information), the file using them must be compiled with C++. This is somewhat unfortunate, because many C programs will use global variables as a convenient means of accessing program data. In almost every case, though, a C program can simply be compiled as C++ by changing the extension to .cc or .cpp.</p>
<h2><a class="anchor" id="subsec_issues_mpi"></a>
MPI</h2>
<p>Everything from MPI 2 is implemented with a few exceptions noted below. The following are <em>not</em> implemented (categorized by MPI concepts):</p>
<h3><a class="anchor" id="subsubsec_issues_mpi_comm"></a>
Communicators</h3>
<ul>
<li>
Anything using or having to do with Inter-communicators (<code>MPI_Intercomm_create()</code>) </li>
<li>
<p class="startli">Topology communicators</p>
<p class="endli"></p>
</li>
</ul>
<h3><a class="anchor" id="subsubsec_issues_mpi_types"></a>
Datatypes and Addressing</h3>
<ul>
<li>
Complicated use of MPI_LB and MPI_UB to define a struct, and collections of structs (MPI test 138). </li>
<li>
Changing the name of built-in datatypes with <code>MPI_Type_set_name()</code> (MPI test 171). </li>
<li>
<code>MPI_Create_darray()</code>, <code>MPI_Create_subarray()</code>, and <code>MPI_Create_resized()</code> </li>
<li>
<code>MPI_Pack_external() </code>, which is only useful for sending messages across MPI implementations apparently. </li>
<li>
<code>MPI_Type_match_size() </code> - extended fortran support </li>
<li>
Use of MPI_BOTTOM (relative addressing). Use normal buffers. </li>
<li>
<p class="startli">Using Fortran types (e.g. MPI_COMLEX) from C.</p>
<p class="endli"></p>
</li>
</ul>
<h3><a class="anchor" id="subsubsec_issues_mpi_info"></a>
Info and Attributes</h3>
<p>No <code>MPI_Info_*</code>, <code>MPI_*_keyval</code>, or <code>MPI_Attr_*</code> functions are supported.</p>
<h3><a class="anchor" id="subsubsec_issues_mpi_ptpt"></a>
Point-to-Point</h3>
<ul>
<li>
<code>MPI_Grequest_*</code> functions (generalized requests). </li>
<li>
<p class="startli">Use of testing non-blocking functions in a loop, such as:</p>
<div class="fragment"><div class="line"><span class="keywordflow">while</span>(!flag)</div><div class="line">{</div><div class="line">  MPI_Iprobe( 0, 0, MPI_COMM_WORLD, &amp;flag, &amp;status );</div><div class="line">}</div></div><!-- fragment --><p>For some configurations, simulation time never advances in the MPI_Iprobe call. This causes an infinite loop that never returns to the discrete event manager. Even if configured so that time progresses, the code will work but will take a very long time to run.</p>
<p class="endli"></p>
</li>
</ul>
<h3><a class="anchor" id="subsubsec_issues_mpi_collectives"></a>
Collectives</h3>
<ul>
<li>
There seems to be a problem with using MPI_FLOAT and MPI_PROD in <code>MPI_Allreduce()</code> (MPI test 22) </li>
<li>
There seems to be a problem with using non-commutative user-defined operators in <code>MPI_Reduce()</code> and <code>MPI_Allreduce()</code>. </li>
<li>
<code>MPI_Alltoallw()</code> is not implemented </li>
<li>
<code>MPI_Exscan()</code> is not implemented </li>
<li>
<code>MPI_Reduce_Scatter_block()</code> is not implemented. </li>
<li>
<code>MPIX_*</code> functions are not implemented (like non-blocking collectives). </li>
<li>
<p class="startli">Calling MPI functions from user-defined reduce operations (MPI test 39; including <code>MPI_Comm_rank</code>).</p>
<p class="endli"></p>
</li>
</ul>
<h3><a class="anchor" id="subsubsec_issues_mpi_misc"></a>
Miscellaneous</h3>
<ul>
<li>
<p class="startli"><code>MPI_Is_thread_main()</code> is not implemented.</p>
<p class="endli"></p>
</li>
</ul>
<h2><a class="anchor" id="subsec_issues_shmem"></a>
OpenSHMEM</h2>
<p>Only the collect and fcollect functions of the API are not implemented (they will be in future releases).</p>
<p>Also, like handling of global variables discussed in Sections <a class="el" href="page_Introduction.html#subsec_issues_globals">Global Variables</a> in the PDF manual, SHMEM globals need to use a different type. For primitives and primitive arrays, refer to <code>&lt;sstmac/openshmem/shmem/globals.h&gt;</code> for replacing types, e.g. the following code</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;int my_global_var = 4;</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;double an_array[6];</div></div><!-- fragment --><p>needs to become</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;shmem_int my_global_var(4);</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;shmem_arr&lt;double, 6&gt; an_array;</div></div><!-- fragment --><h2><a class="anchor" id="subsec_issues_fortran"></a>
Fortran</h2>
<p>SST/macro can run Fortran90 applications. However, at least using gfortran, Fortran variables using allocate() go on the heap. Therefore, it creates a false sharing situation pretty much everywhere as threads swap in and out. A workaround is to make a big map full of data structures that store needed variables, indexed by a rank that you pass around to every function. We are exploring more user-friendly alternatives. The Fortran MPI interface is also still somewhat incomplete. Most functions are just wrappers to the C/C++ implementation and we are working on adding the bindings. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
